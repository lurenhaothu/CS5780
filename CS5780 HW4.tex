\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{amsmath}

\title{CS5780 HW4}
\author{Renhao Lu, NetID: rl839}
	
\begin{document}
	\maketitle
	
	\section{Problem1: Linear Regression}
	\subsection{Compute the closed form solution for $\textbf{w}$}
	Using the formula in class:
	\[
	\textbf{w} = 
		(\textbf{X} \textbf{X}^T) ^{-1}
		\textbf{X}
			\textbf{y}^T
	\]
	where $\textbf{X}=\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\ -1 & 0 & 1 & 2 & 3\end{pmatrix}$ and  $\textbf{y}=\begin{pmatrix} 4 & 3 & -4& 3 & 7 \end{pmatrix}$\\
	Hence,
	\[\begin{split}
		\textbf{w}&=(\textbf{X}\textbf{X}^T)^{-1}\textbf{X}\textbf{y}^T\\
		&=\begin{pmatrix} 5 & 5 \\ 5 & 15\end{pmatrix}^{-1}\textbf{X}\textbf{y}^T\\
		&=\begin{pmatrix} 0.3 & -0.1 \\ -0.1 & 0.1\end{pmatrix}\textbf{X}\textbf{y}^T\\
		&=\begin{pmatrix} 0.4 & 0.3 & 0.2 & 0.1 & 0 \\ -0.2 & -0.1 & 0 & 0.1 & 0.2\end{pmatrix}\textbf{y}^T\\
		&=\begin{pmatrix} 2 \\ 0.6\end{pmatrix}
	\end{split}\]
	
	\subsection{Calculate the training loss}
	\[
		\begin{split}
			l(\textbf{w})&=\sum (y_i-\textbf{w}^T\phi(x_i)^2)\\
			&={||\textbf{y}-\textbf{w}^textbf{X}||}_2^2\\
			&={||\begin{pmatrix} 2.6 & 1 & -6.6 & -0.2 & 3.2 \end{pmatrix}||}_2^2\\
			&=61.6
		\end{split}
	\]
	
	\section{Problem: Linearity of Gaussian Naive Bayes}
	\subsection{Show that the decision rule}
	According to the law of total probability:
	\[
		\begin{split}
			p(\textbf{x}) &= p(y=1)p(\textbf{x}|y=1)+p(y=0)p(\textbf{x}|y=0)\\
			&=p(y=1)\prod_{\alpha=1}^d {p([\textbf{x}]_{\alpha}|y=1)} +p(y=0)\prod_{\alpha=1}^d {p([\textbf{x}]_{\alpha}|y=0)}
		\end{split}
	\]
	Hence,
	\[
		\begin{split}
			p(y=1|\textbf{x}) &=\frac{
				p(y=1) 
				\prod_{\alpha=1}^d {
					p([\textbf{x}]_{\alpha}|y=1)
				} 
			}{
				p(\textbf{x})
			} \\
			&=\frac{
				p(y=1)\prod_{\alpha=1}^d {
					p([\textbf{x}]_{\alpha}|y=1)
				}
			}{
				p(y=1)\prod_{\alpha=1}^d {
					p([\textbf{x}]_{\alpha}|y=1)
				}
				+p(y=0)\prod_{\alpha=1}^d {
					p([\textbf{x}]_{\alpha}|y=0)
				}
			}
		\end{split}
	\]
	
	\subsection{Show how to rewrite}
	From the formula we got from 1:
	\[
		\begin{split}
			p(y=1|\textbf{x}) 
			&=\frac{
				p(y=1)\prod_{\alpha=1}^d {
					p([\textbf{x}]_{\alpha}|y=1)
				}
			}{
				p(y=1)\prod_{\alpha=1}^d {
					p([\textbf{x}]_{\alpha}|y=1)
				}
				+p(y=0)\prod_{\alpha=1}^d {
					p([\textbf{x}]_{\alpha}|y=0)
				}
			}\\
			&=\frac{1}{
				1 + \frac{
					p(y=0)\prod_{\alpha=1}^d {
						p([\textbf{x}]_{\alpha}|y=0)
					}
				}{
					p(y=1)\prod_{\alpha=1}^d {
						p([\textbf{x}]_{\alpha}|y=1)
					}
				}
			}\\
			&=\frac{1}{
				1 + exp(
					-log(
						\frac{
							p(y=1)\prod_{\alpha=1}^d {
								p([\textbf{x}]_{\alpha}|y=1)
							}
						}{
							p(y=0)\prod_{\alpha=1}^d {
								p([\textbf{x}]_{\alpha}|y=0)
							}
						}
					)
				)
			}\\
		\end{split}
	\]
	\subsection{Show that Naive Bayes is a linear model}
	Because
	\[
		p([\textbf{x}]_{\alpha}|y=1)=
		\frac{1}{\sqrt{2\pi[\sigma_{\alpha}]}}
		\exp\left({
			\frac{
			-([\textbf{x}]_{\alpha}-[\mu_1]_{\alpha})^2
			}{
				2[\sigma]_{\alpha}
			}
		}\right)
	\]
	\[
		p([\textbf{x}]_{\alpha}|y=0)=
		\frac{1}{\sqrt{2\pi[\sigma_{\alpha}]}}
		\exp\left({
			\frac{
			-([\textbf{x}]_{\alpha}-[\mu_0]_{\alpha})^2
			}{
				2[\sigma]_{\alpha}
			}
		}\right)
	\]
	Hence,
	\[
		\begin{split}
			p(y=1|\textbf{x})&=\frac{1}{1+\frac{p(y=0)}{p(y=1)}\exp{\left(\sum_{\alpha=1}^d{-\log{\frac{p([\textbf{x}]_\alpha|y=1)}{p([\textbf{x}]_\alpha|y=0)}}}\right)}}\\
			&=\frac{1}{1+\frac{p(y=0)}{p(y=1)}\exp{\left(\sum_{\alpha=1}^d{\frac{([\textbf{x}]_{\alpha}-[\mu_0]_{\alpha})^2-([\textbf{x}]_{\alpha}-[\mu_1]_{\alpha})^2}{2[\sigma]_{\alpha}}}\right)}}\\
			&=\frac{1}{1+\frac{p(y=0)}{p(y=1)}\exp{\left(\sum_{\alpha=1}^d{\frac{2([\mu_1]_{\alpha}- [\mu_0]_{\alpha})[\textbf{x}]_{\alpha}+([\mu_0]_{\alpha}^2-[\mu_1]_{\alpha}^2)}{2[\sigma]_{\alpha}}}\right)}}
		\end{split}
	\]
	We assume:
	\[\textbf{w}=[w_1, w_2,...,w_b], w_\alpha=\frac{[\mu_1]_{\alpha}- [\mu_0]_{\alpha}}{[\sigma]_{\alpha}}\]
	\[b=\sum{b_\alpha}+b_0, b_0=\log{\frac{p(y=0)}{p(y=1)}}, b_\alpha=\frac{[\mu_0]_{\alpha}^2-[\mu_1]_{\alpha}^2}{2[\sigma]_{\alpha}}\]
	Hence,
	\[\begin{split}
		p(y=1|\textbf{x})&=\frac{1}{1+\exp{\left(b_0+\sum_{\alpha=1}^d {(w_\alpha[\textbf{x}]_\alpha+b_\alpha)}\right)}}\\
		&=\frac{1}{1+\exp{\left(\textbf{w}^T\textbf{x}+b\right)}}
	\end{split}\]
	From the formula above, whether $p(y=1|\textbf{x}) > p(y=0|\textbf{x})$ is decided by whether $\textbf{w}^T\textbf{x}+b>0$, hence this Naive Bayes is a linear model.
	
	\section{Gradient for Logistic Regression}
	\subsection{Show the sigmoid function property}
	\[\begin{split}
		\sigma(s)+\sigma(-s)&=\frac{1}{1+e^{-s}}+\frac{1}{1+e^{s}}\\
		&=\frac{e^s}{1+e^{s}}+\frac{1}{1+e^{s}}=1
	\end{split}\]
	Hence, $\sigma(-s)=1-\sigma(s)$
	
	\subsection{Compute the gradient of the log likelihood function}
	\subsubsection{first derivative}
	\[\sigma'(s)=\]
\end{document}